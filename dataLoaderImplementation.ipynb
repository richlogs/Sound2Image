{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "num_workers = 0\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "embeddingPath = \"Embeddings/audio/subURMPClean/train/all_embeddings.pt\"\n",
    "embeddingNamePath = \"Embeddings/audio/subURMPClean/train/all_file_names.csv\"\n",
    "dataset_path = \"Data/SubURMP64/images/clean\"\n",
    "train_folder = \"trial\"\n",
    "val_folder = \"trial\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_path, embeddingPath, namePath, transform=None):\n",
    "        \n",
    "\n",
    "        self.image_path = image_path # Set image path e.g. trial for demonstration, train for application\n",
    "        self.transform = transform\n",
    "        file_list = glob.glob(self.image_path + \"*\")\n",
    "\n",
    "        # Getting image names\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
    "                self.data.append(img_path)\n",
    "\n",
    "        # Reading in embedding file and associated names\n",
    "        self.embeddingArr = torch.load(embeddingPath)\n",
    "        self.fileNames = np.loadtxt(namePath, delimiter=',', dtype=str)\n",
    "\n",
    "\n",
    "\n",
    "        self.img_dim = (64, 64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]  # Gets path to image\n",
    "\n",
    "        with Image.open(img_path) as img:  # Loads image\n",
    "            img.load()\n",
    "        img = img.convert(\"RGB\")  # Converts image to rgb\n",
    "        img_path = img_path.split(\"/\")[-1]\n",
    "        #TODO use img_path to lookup embedding from embedding file created in __init__\n",
    "\n",
    "        embeddingIdx = np.where(self.fileNames == img_path)  # Index for embeddings where it corresponds to the desired file name\n",
    "        embedding = self.embeddingArr[embeddingIdx]  # Embeddings for associated index\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, embedding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Any paramater in get_data() should be found in args in actual implementation\n",
    "def get_data(img_size, dataset_path, embeddingPath, namePath, train_folder, val_folder, batch_size, num_workers):  # Defines dataloaders and transformations for data\n",
    "    train_transforms = torchvision.transforms.Compose([\n",
    "        T.Resize(img_size + int(.25*img_size)),  # args.img_size + 1/4 *args.img_size\n",
    "        T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    val_transforms = torchvision.transforms.Compose([\n",
    "        T.Resize(img_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "\n",
    "    train_dataset = CustomDataset(image_path=f\"{dataset_path}/{train_folder}/\", embeddingPath=embeddingPath, namePath=namePath,transform=train_transforms)\n",
    "    val_dataset = CustomDataset(image_path=f\"{dataset_path}/{val_folder}/\", embeddingPath=embeddingPath, namePath=namePath,transform=val_transforms)\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers) #Defines the train dataloader\n",
    "    val_dataset = DataLoader(val_dataset, batch_size=2*batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_dataloader, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_data(img_size, dataset_path, embeddingPath, embeddingNamePath, \n",
    "                                            train_folder, val_folder, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bassoon00_22700.jpg', 'cello00_5800.jpg')]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgsList = []\n",
    "labsList = []\n",
    "embList = []\n",
    "for imgs, labs, emb in train_dataloader:\n",
    "    imgsList.append(imgs)\n",
    "    labsList.append(labs)\n",
    "    embList.append(emb)\n",
    "\n",
    "labsList\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next piece of the puzzle \n",
    "\n",
    "The next part of the source code that is relevant is the one_epoch method in ddpm_conditional. Specifically line 118. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
